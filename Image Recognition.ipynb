{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with CNN on MNIST\n",
        "\n",
        "Recognize handwritten digits in the MNIST dataset and evaluate the model's performance.\n",
        "\n",
        "## Data Description\n",
        "- MNIST dataset consisting of grayscale images of handwritten digits (0-9).\n",
        "\n",
        "## Model Overview\n",
        "- CNN architecture with convolutional and fully connected layers.\n",
        "\n",
        "## Training Procedure\n",
        "- Splitting data into training and testing sets.\n",
        "- Training the model on the training set and evaluating on the test set.\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "ZqLU6WE9MqCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOI4ORDRMoQZ",
        "outputId": "1ebe88e1-4140-41c0-9ddc-997dd6adac36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics import Accuracy, Precision, Recall\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "1B3w45yQnUtr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*    converts images from the MNIST dataset into PyTorch tensors and then download  MNIST dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lh0dQ9zWEJ5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "miZmYF2BMlwQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write some code to know the information about your data"
      ],
      "metadata": {
        "id": "WFZ4Sdg-G56X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(train_dataset)\n",
        "print(\"Size of the entire dataset:\", dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BflpkLXnNBvH",
        "outputId": "92b3c967-26bb-4ef2-86c0-d94dffa27c0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the entire dataset: 60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  the number above is number of images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bbjh_TVnHHF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image = train_dataset[0][0].shape\n",
        "print(\"Shape of a single data point:\", first_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wut0KSptN_Ws",
        "outputId": "1f8f8307-6176-4357-bd05-30a5abe20f1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of a single data point: torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The first dimension (1) represents the number of channels.\n",
        "*   The second dimension (28) corresponds to the height of the image.\n",
        "*   The third dimension (28) corresponds to the width of the image.:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cEA201pWHda2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loaders efficiently load and provide batches of data during the training and testing phases of a neural network."
      ],
      "metadata": {
        "id": "6iJRoWAAJPib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader =  DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "aYV5copkng6l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show some of images from the data\n",
        "for images, labels in train_loader:\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "    for idx in range(4):\n",
        "        axes[idx].imshow(images[idx].squeeze().numpy(), cmap='gray')\n",
        "        axes[idx].axis('off')\n",
        "        axes[idx].set_title(f'Label: {labels[idx].item()}')\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "O7ZyRBjYN0Jz",
        "outputId": "a99bae81-27de-4a6a-d73b-eeb9446421ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Convolutional Neural Network"
      ],
      "metadata": {
        "id": "ZJ1S1x-LJYOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(in_features=64 * 7 * 7, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "your_cnn = CNN(num_classes)\n",
        "\n",
        "print(your_cnn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5S9EzxdXoWF",
        "outputId": "c70ee978-7517-4b99-cfa3-1aefa68c6125"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (feature_extractor): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ELU(alpha=1.0)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ELU(alpha=1.0)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (classifier): Linear(in_features=3136, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the CNN"
      ],
      "metadata": {
        "id": "bTXEDYxWK274"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(optimizer, net, num_epochs):\n",
        "    num_processed = 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0\n",
        "        num_processed = 0\n",
        "        for features, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            num_processed += len(labels)\n",
        "        print(f'epoch {epoch}, loss: {running_loss / num_processed}')\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "net = CNN(num_classes)\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "train_model(\n",
        "    optimizer=optimizer,\n",
        "    net=net,\n",
        "    num_epochs=5,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0lLtOtZa_H4",
        "outputId": "60ebf88a-cb41-4948-b640-8fc9d5a83b9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss: 0.004503149253389954\n",
            "epoch 1, loss: 0.0016686253464989326\n",
            "epoch 2, loss: 0.0012737755285884\n",
            "epoch 3, loss: 0.001037742068260862\n",
            "epoch 4, loss: 0.0008250852113877954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the CNN"
      ],
      "metadata": {
        "id": "sX4Xwp96LBC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.eval()\n",
        "predicted = []\n",
        "true_labels = []\n",
        "\n",
        "for i, (features, label) in enumerate(test_dataset):\n",
        "    output = net.forward(features.reshape(-1, 1, 28, 28))\n",
        "    cat = torch.argmax(output, dim=-1)\n",
        "    predicted.append(cat.item())\n",
        "    true_labels.append(label)\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted)\n",
        "precision = precision_score(true_labels, predicted, average=None)\n",
        "recall = recall_score(true_labels, predicted, average=None)\n",
        "\n",
        "print('Accuracy: {:.2%}'.format(accuracy))\n",
        "print('Precision (per class):', precision)\n",
        "print('Recall (per class):', recall)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JOKV6I7cQ7E",
        "outputId": "f5f119e3-8591-4f46-c533-25b0aeb92353"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.27%\n",
            "Precision (per class): [0.98274112 0.9955157  0.98346304 0.98422091 0.99179487 0.97888889\n",
            " 0.99055614 0.99007937 0.95427435 0.97440945]\n",
            "Recall (per class): [0.9877551  0.97797357 0.97965116 0.98811881 0.98472505 0.98766816\n",
            " 0.98538622 0.97081712 0.98562628 0.98116947]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we implemented and trained a convolutional neural network (CNN) on the MNIST dataset. The model achieved an impressive accuracy of 98.27% on the test set. The precision and recall scores per class demonstrate the model's effectiveness in recognizing various digits\n",
        "\n",
        "Thank you for following along!"
      ],
      "metadata": {
        "id": "890O_nxqLY_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Re7lTeJNLFF1"
      }
    }
  ]
}